{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfYXGXE+4KUUj95JYhYNDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rupaut98/LLM_Oreilly/blob/main/ch1_ch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgG3Y-HASqd2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "device_map=\"cuda\",\n",
        "torch_dtype=\"auto\",\n",
        "trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"<s> Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "input_ids=input_ids,\n",
        "max_new_tokens=20\n",
        ")\n",
        "# Print the output\n",
        "print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "id": "ta4FWztoUqMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in input_ids[0]:\n",
        "  print(tokenizer.decode(id))"
      ],
      "metadata": {
        "id": "tGaMbZLsVJ1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generation_output)"
      ],
      "metadata": {
        "id": "fggl9DBGVLzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode([278,25305, 293, 16423]))\n",
        "print(tokenizer.decode(622))\n",
        "print(tokenizer.decode([3323, 622]))\n",
        "print(tokenizer.decode(29901))"
      ],
      "metadata": {
        "id": "_YQVb6zBWKyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "ðŸŽµé¸Ÿ\n",
        "show_tokens False None elif == >= else: two tabs:\" \" Three tabs: \" \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "l02C0Pz8WQVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors_list = [\n",
        "'102;194;165', '252;141;98', '141;160;203',\n",
        "'231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "  token_ids = tokenizer(sentence).input_ids\n",
        "  for idx, t in enumerate(token_ids):\n",
        "    print(\n",
        "    f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "    tokenizer.decode(t) +\n",
        "    '\\x1b[0m',\n",
        "    end=' '\n",
        "    )"
      ],
      "metadata": {
        "id": "J6h45cw-9dLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer):\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode([t]) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "ðŸŽµé¸Ÿ\n",
        "show_tokens False None elif == >= else: two tabs:\" \" Three tabs: \" \"\n",
        "12.0*50=600\n",
        "\"\"\"\n",
        "\n",
        "# Show tokens for the input text\n",
        "show_tokens(text, tokenizer)\n"
      ],
      "metadata": {
        "id": "cbLdheqJ9rWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "# Load a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "# Load a language model\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer('Hello world', return_tensors='pt')\n",
        "# Process the tokens\n",
        "output = model(**tokens)[0]"
      ],
      "metadata": {
        "id": "PI7t1BEAEizx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "vhomOIaYEnRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens['input_ids'][0]:\n",
        "  print(tokenizer.decode(token))"
      ],
      "metadata": {
        "id": "icfchRCKEpnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# Load model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "# Convert text to text embeddings\n",
        "vector = model.encode(\"Best movie ever!\")"
      ],
      "metadata": {
        "id": "twcf03SzFSsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector.shape"
      ],
      "metadata": {
        "id": "CpYKx-TcFvGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from urllib import request\n",
        "# Get the playlist dataset file\n",
        "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
        "# Parse the playlist dataset file. Skip the first two lines as\n",
        "# they only contain metadata\n",
        "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
        "# Remove playlists with only one song\n",
        "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
        "# Load song metadata\n",
        "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
        "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
        "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
        "songs_df = songs_df.set_index('id')"
      ],
      "metadata": {
        "id": "aU6XRPMaGife"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( 'Playlist #1:\\n ', playlists[0], '\\n')\n",
        "print( 'Playlist #2:\\n ', playlists[1])"
      ],
      "metadata": {
        "id": "Xi3rBty_Qybf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "\"microsoft/Phi-3-mini-4k-instruct\"\n",
        ",\n",
        "device_map=\n",
        "\"cuda\"\n",
        ",\n",
        "torch_dtype=\n",
        "\"auto\"\n",
        ",\n",
        "trust_remote_code=True,\n",
        ")\n",
        "\n",
        "generator = pipeline(\n",
        "    'text-generation',\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    return_full_text = False,\n",
        "    max_new_tokens = 50,\n",
        ")"
      ],
      "metadata": {
        "id": "1VucaciHQ0vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happen\"\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(\"cuda\")"
      ],
      "metadata": {
        "id": "62IXI9Yn8vkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "input_ids=input_ids,\n",
        "max_new_tokens=50,\n",
        "use_cache=False\n",
        ")"
      ],
      "metadata": {
        "id": "Dj3LRYQi87Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hQqqTwfrv33a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "data"
      ],
      "metadata": {
        "id": "pCWMFLnKOPOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"train\"][0, -1]"
      ],
      "metadata": {
        "id": "Fic870ZRObgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a pretrained model to classify sentiments\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "model_path = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
        "\n",
        "pipe = pipeline(\n",
        "    model = model_path,\n",
        "    tokenizer = model_path,\n",
        "    return_all_scores = True,\n",
        "    device = \"cuda:0\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "omvLy6AwOoAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "y_pred = []\n",
        "\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total = len(data[\"test\"])):\n",
        "  negative_score = output[0][\"score\"]\n",
        "  positive_score = output[2][\"score\"]\n",
        "  assignment = np.argmax([negative_score, positive_score])\n",
        "  y_pred.append(assignment)"
      ],
      "metadata": {
        "id": "f83oV7pbRjFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_performance(y_true, _pred):\n",
        "\n",
        "  performance = classification_report(\n",
        "      y_true, y_pred,\n",
        "      target_names = [\"Negative Review\", \"Positive Review\"]\n",
        "  )\n",
        "  print(performance)\n"
      ],
      "metadata": {
        "id": "9xzNXqXLTeMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What if we don't have a pretrained model for sentiments. Well, we don't really need to finetune the model and train it which would be\n",
        "#computationally expensive, rather we can use a pretrained embedding model like BERT or sentence-transformers\n",
        "#and then use it to train a classifier model using logistic regression thus using CPU instead of GPU\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "train_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_in_bar = True)\n",
        "test_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_in_bar = True)"
      ],
      "metadata": {
        "id": "jvCcFU1MUZqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings[0].shape"
      ],
      "metadata": {
        "id": "zqA39vDtW6Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state = 42)\n",
        "clf.fit(train_embeddings, data[\"train\"][\"label\"])\n"
      ],
      "metadata": {
        "id": "sX7UT7Ghe1Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(test_embeddings)\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ],
      "metadata": {
        "id": "-SX5XdpGgQBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_embeddings = model.encode([\"A negative review\", \"A positive review\"])"
      ],
      "metadata": {
        "id": "e80KNlaTjM_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_embeddings[0].shape"
      ],
      "metadata": {
        "id": "3frmxALckB3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sim_matrix = cosine_similarity(test_embeddings, label_embeddings)\n",
        "sim_matrix[0]\n",
        "y_pred = np.argmax(sim_matrix, axis = 1)"
      ],
      "metadata": {
        "id": "G4vMbngJkFNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_matrix[0]"
      ],
      "metadata": {
        "id": "Vo1VSECJkxYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[0]"
      ],
      "metadata": {
        "id": "JZjFztoIky6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ],
      "metadata": {
        "id": "w1I-Cc4Vk6rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using a generative model (with both decoder and encoding layers) instead of just models with encoding lyaers only\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model = \"google/flan-t5-small\",\n",
        "    device = \"cuda:0\"\n",
        ")"
      ],
      "metadata": {
        "id": "9uvNCnZclLs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Is the following sentence positive or negative\"\n",
        "data = data.map(lambda example: {\"t5\": prompt + example['text']})\n",
        "data[\"train\"][0]"
      ],
      "metadata": {
        "id": "ujfKPYEcoAf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "y_pred = []\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), total = len(data[\"test\"])):\n",
        "  print(output)\n",
        "  text = output[0][\"generated_text\"]\n",
        "  y_pred.append(0 if text == \"negative\" else 1)"
      ],
      "metadata": {
        "id": "Ab0cYplNoS4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ],
      "metadata": {
        "id": "hPC3f5sGqGZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "4EqB3BGarPv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "client = openai.OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "wlzsbuvBsTkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_generation(prompt, document, model = \"gpt-3.5-turbo-0125\"):\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant\"\n",
        "        },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": prompt.replace(\"[DOCUMENT]\", document)\n",
        "      }\n",
        "  ]\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      messages = messages,\n",
        "      model = model,\n",
        "      temperature = 0\n",
        "  )\n",
        "  return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "g5eUg10osho-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Predict whether the following document is a positive or negative movie review:\n",
        "[DOCUMENT]\n",
        "If it is positive return 1 and if it is negative return 0. Do not give any other answers.\n",
        "\"\"\"\n",
        "document = \"unpretentious , charming , quirky , original\"\n",
        "chatgpt_generation(prompt, document)"
      ],
      "metadata": {
        "id": "atRboNlbtfoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ThN72fg_trj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}