{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install langchain-community\nfrom langchain_community.llms import LlamaCpp\n\nllm = LlamaCpp(\n    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n    n_gpu_layers=50,\n    max_tokens=500,\n    n_ctx=2048,\n    seed=42,\n    verbose=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain import PromptTemplate\n\ntemplate = \"\"\"<s><|user|>\n{input_prompt}<|end|>\n<|assistant|>\"\"\"\n\nprompt = PromptTemplate(template = template, input_variables = [\"input_prompt\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basic_chain = prompt | llm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basic_chain.invoke(\n    {\n        \"input_prompt\":\"Hi! My name is Maarten. What is 1 + 1?\"\n    })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain import LLMChain\nfrom langchain import PromptTemplate\n\ntemplate = \"\"\"<s><|user|>\nCreate a title for a story about {summary}. Only return the title.<|end|>\n<|assistant|>\"\"\"\n\ntitle_prompt = PromptTemplate(template = template, input_variables = [\"summary\"])\ntitle = LLMChain(llm = llm, prompt=title_prompt, output_key = \"title\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"<s><|user|>\nDescribe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n<|assistant|>\"\"\"\n\ncharacter_prompt = PromptTemplate(template = template, input_variables=['summary', 'title'])\ncharacter = LLMChain(llm = llm, prompt = character_prompt, output_key = \"character\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template =\"\"\"<s><|user|>\nCreate a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragraph\n<|assistant|>\"\"\"\nstory_prompt = PromptTemplate(template=template, input_variables=[\"summary\",\"title\",\"character\"])\nstory = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_chain = title | character | story","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basic_chain.invoke({\"input_prompt\":\"Hi my name is Rupak. What is an apple?\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basic_chain.invoke({\"input_prompt\":\"Hey What's my name?\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"<s><|user|>Current conversation:{chat_history}\n\n{input_prompt}<|end|>\n<|assistant|>\"\"\"\n\nprompt = PromptTemplate(\ntemplate = template,\ninput_variables = [\"input_prompt\", \"chat_history\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\n\nllm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_chain.invoke({\"input_prompt\": \"Hi! My name is Rupal. What is 1 + 1?\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_chain.invoke({\"input_prompt\":\"Hi! What is my name?\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.memory import ConversationBufferWindowMemory\n# Retain only the last 2 conversations in memory\nmemory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n\nllm_chain = LLMChain(\nprompt = prompt,\nllm=llm,\nmemory=memory)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_chain.predict(input_prompt=\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\")\nllm_chain.predict(input_prompt=\"What is 3 + 3?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_chain.invoke({\"input_prompt\":\"What is my name?\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_chain.invoke({\"input_prompt\":\"What is my age?\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain import PromptTemplate\nsummary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n\nCurrent Summary:\n{summary}\n\nnew lines of conversation:\n{new_lines}\n\nNew summary:<|end|>\n<|assistant|>\"\"\"\n\nsummary_prompt = PromptTemplate(\ninput_variables = [\"summary\", 'new_lines'],\ntemplate = summary_prompt_template,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.memory import ConversationSummaryMemory\nfrom langchain import LLMChain\n\nmemory = ConversationSummaryMemory(\nllm = llm,\nmemory_key = \"chat_history\",\nprompt = summary_prompt)\n\nllm_chain = LLMChain(\nprompt = prompt,\nllm=llm,\nmemory = memory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"memory.load_memory_variables({})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LLM Agents","metadata":{}},{"cell_type":"code","source":"\n\nopenai_llm = ChatOpenAI(api_key=\"\", model_name = 'gpt-3.5-turbo', temperature = 0)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T18:39:26.525793Z","iopub.execute_input":"2024-10-15T18:39:26.526415Z","iopub.status.idle":"2024-10-15T18:39:26.555117Z","shell.execute_reply.started":"2024-10-15T18:39:26.526373Z","shell.execute_reply":"2024-10-15T18:39:26.554133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_core.prompts import PromptTemplate\nreact_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: {input}\nThought:{agent_scratchpad}\"\"\"\n\nprompt = PromptTemplate(\ntemplate = react_template,\ninput_variables = [\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-15T18:29:17.384949Z","iopub.execute_input":"2024-10-15T18:29:17.385809Z","iopub.status.idle":"2024-10-15T18:29:17.391144Z","shell.execute_reply.started":"2024-10-15T18:29:17.385769Z","shell.execute_reply":"2024-10-15T18:29:17.390108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.agents import load_tools, Tool\nfrom langchain.tools import DuckDuckGoSearchResults\n\n# You can create the tool to pass to an agent\nsearch = DuckDuckGoSearchResults()\nsearch_tool = Tool(\n    name=\"duckduck\",\n    description=\"A web search engine. Use this to as a search engine for general queries.\",\n    func=search.run,\n)\n\n# Prepare tools\ntools = load_tools([\"llm-math\"], llm=openai_llm)\ntools.append(search_tool)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T18:43:03.633310Z","iopub.execute_input":"2024-10-15T18:43:03.634154Z","iopub.status.idle":"2024-10-15T18:43:03.640907Z","shell.execute_reply.started":"2024-10-15T18:43:03.634113Z","shell.execute_reply":"2024-10-15T18:43:03.639922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tools = load_tools([\"llm-math\"], llm=openai_llm)\ntools.append(search_tool)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T18:43:05.453144Z","iopub.execute_input":"2024-10-15T18:43:05.453840Z","iopub.status.idle":"2024-10-15T18:43:05.459030Z","shell.execute_reply.started":"2024-10-15T18:43:05.453795Z","shell.execute_reply":"2024-10-15T18:43:05.457992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.agents import AgentExecutor, create_react_agent\n\nagent = create_react_agent(openai_llm, tools, prompt)\n\nagent_executor = AgentExecutor(\nagent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T18:43:07.696522Z","iopub.execute_input":"2024-10-15T18:43:07.697161Z","iopub.status.idle":"2024-10-15T18:43:07.702901Z","shell.execute_reply.started":"2024-10-15T18:43:07.697119Z","shell.execute_reply":"2024-10-15T18:43:07.701927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What is the Price of a MacBook Pro?\nagent_executor.invoke(\n    {\n        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T18:43:42.445128Z","iopub.execute_input":"2024-10-15T18:43:42.446109Z","iopub.status.idle":"2024-10-15T18:43:50.869676Z","shell.execute_reply.started":"2024-10-15T18:43:42.446061Z","shell.execute_reply":"2024-10-15T18:43:50.868723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}